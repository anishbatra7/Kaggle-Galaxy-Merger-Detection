{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-GY 9223-D: Deep Learning Homework 2\n",
    "\n",
    "## Documentation\n",
    "\n",
    "Member 1: Anish Batra, ab8166\n",
    "\n",
    "Member 2: Prashant Mahajan, prm349"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€“ Duration of training: approx 3 hours each for both ResNet50 and Xception (30 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16: \n",
    "\n",
    "- Started with VGG16 base kernel. Made changes in the dataset to get validation dataset. Achieved Score - 0.12501\n",
    "- Implemented offline data augmentation (200,000 images), and trained using VGG16 model. Achieved Score - 0.11754\n",
    "\n",
    "### Resnet50:\n",
    "\n",
    "- Resnet50 without training initial layers gave accuracy of 0.09780. \n",
    "- Training all the layers gave boost till 0.06251. \n",
    "- Added Keras callback in the model to give best weights for each 3rd iteration. Score - 0.05282\n",
    "- Tried improving the model with different optimizers. List of Optimizers experimented with - SGD, adam, nadam, RMSE, AdaBound, AdamW. All of these either produced lower or similar validation accuracy.\n",
    "- You can find more information on: [AdaBound](https://github.com/Luolc/AdaBound) and [AdamW](https://github.com/loshchil/AdamW-and-SGDW).  \n",
    "\n",
    "### Xception:\n",
    "\n",
    "- Fine tuning of all layers, adam with decay, no zooming in images, online data augmentation - mainly rotation with NVIDIA V100 GPU, 16 CPUs. Boost till 0.03697. \n",
    "- Further tried to improve the model with different optimizers, changes in data augmentation methods (zooming, ZCA whitening etc), changing no. of epochs. Couldn't beat the previous score. Score achieved in the range of 0.04234 - 0.04533\n",
    "- Since adam showed the most promising results - we trained our Xception model for a really long time - 30 epochs. \n",
    "\n",
    "### Ensembling (Resnet50 & Xception):\n",
    "\n",
    "- Ensemble of two Xception models showed significant improvement in the result. Achieving 4th rank in private leadboard of original Galaxy Zoo competition.\n",
    "- Seeing the improvement we decided to go with Resnet50 + Xception. Both trained for 30 epochs with similar configurations. Achieved score of 0.03503 placing us on the top (yayy)\n",
    "\n",
    "### Observations and freebies left unsused due to time limit:\n",
    "\n",
    "- Optimizers and how you implement them according to problem statement plays an important role. AdaBound and AdamW both looked promising and something we would keep in our pockets for future use. \n",
    "- Experimentation with Data Augmentation, batch size and image size - not every feature in ImageDataGenerator is useful. \n",
    "- Ensembles we wanted to try - Densnet + VGG16 + Resnet50 + Xception "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
